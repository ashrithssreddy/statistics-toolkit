{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"table-of-contents\"></a>\n",
    "# 📖 Table of Contents\n",
    "\n",
    "[🗂️ Data Setup](#data-setup)     \n",
    "- [🔍 Generate Data](#generate-data)      \n",
    "- [⚙️ Define Test Configuration](#define-test-config)\n",
    "\n",
    "[🛠️ Test Setup](#test-setup)    \n",
    "- [🧪 Validate Configuration Dictionary](#validate-config)\n",
    "- [📋 Print Config Summary](#print-config)\n",
    "\n",
    "[📈 Inference](#inference)  \n",
    "- [🔍 Infer Distribution From Data](#infer-distribution)      \n",
    "- [📏 Infer Variance Equality](#infer-variance)    \n",
    "- [📊 Infer Parametric Flag](#infer-parametric)    \n",
    "\n",
    "[🧪 Hypothesis Testing](#hypothesis-testing)    \n",
    "- [🧭 Determine Test To Run](#determine-test)    \n",
    "- [🧠 Print Hypothesis Statement](#print-hypothesis)    \n",
    "- [🧪 Run Hypothesis Test](#run-test)    \n",
    "\n",
    "[📊 Test Summary](#test-summary)    \n",
    "\n",
    "[🚀 Full Pipeline](#full-pipeline)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Hypothesis Testing - Assumptions & Methods (Click to Expand)</strong></summary>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Test Type</th>\n",
    "      <th>Use Case</th>\n",
    "      <th>Parametric?</th>\n",
    "      <th>Assumptions</th>\n",
    "      <th>Non-Parametric Alternative</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>One-Sample t-test</td>\n",
    "      <td>Compare sample mean vs. known value</td>\n",
    "      <td>✅</td>\n",
    "      <td>Normality of sample</td>\n",
    "      <td>Sign test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Two-Sample t-test</td>\n",
    "      <td>Compare means of two independent groups</td>\n",
    "      <td>✅</td>\n",
    "      <td>- Normality (both groups)<br>- Equal variance (if pooled)<br>- Independence</td>\n",
    "      <td>Mann-Whitney U</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Paired t-test</td>\n",
    "      <td>Compare means of two related samples (before-after, matched)</td>\n",
    "      <td>✅</td>\n",
    "      <td>- Normality of *differences*<br>- No extreme outliers</td>\n",
    "      <td>Wilcoxon signed-rank</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Proportions z-test</td>\n",
    "      <td>Compare binary rates (e.g., CTR in A vs B)</td>\n",
    "      <td>✅</td>\n",
    "      <td>- np > 5, nq > 5 (sample size rule)<br>- Independence</td>\n",
    "      <td>Fisher’s exact</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Chi-Square Test</td>\n",
    "      <td>Categorical association (e.g., device type vs. AR adoption)</td>\n",
    "      <td>✅</td>\n",
    "      <td>- Expected count ≥ 5 in ≥ 80% of cells<br>- Independence</td>\n",
    "      <td>Fisher’s exact</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>ANOVA</td>\n",
    "      <td>Compare means across 3+ groups</td>\n",
    "      <td>✅</td>\n",
    "      <td>- Normality<br>- Equal variance<br>- Independence</td>\n",
    "      <td>Kruskal-Wallis</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Mann-Whitney U</td>\n",
    "      <td>Compare medians/ranks of two independent groups</td>\n",
    "      <td>❌</td>\n",
    "      <td>- Same shape distribution (ideally)<br>- Ordinal or continuous data</td>\n",
    "      <td>N/A</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Wilcoxon Signed-Rank</td>\n",
    "      <td>Paired version of Mann-Whitney (for related samples)</td>\n",
    "      <td>❌</td>\n",
    "      <td>- Symmetry in differences<br>- Ordinal or continuous</td>\n",
    "      <td>Sign test</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Hypothesis Test Selection Matrix (Click to Expand)</strong></summary>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Outcome Type</th>\n",
    "      <th>Group Relationship</th>\n",
    "      <th>Group Count</th>\n",
    "      <th>Outcome Distribution</th>\n",
    "      <th>Business Problem</th>\n",
    "      <th>Recommended Test</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>–</td>\n",
    "      <td>one-sample</td>\n",
    "      <td>normal</td>\n",
    "      <td>Is average order value different from $50?</td>\n",
    "      <td>One-sample t-test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>binary</td>\n",
    "      <td>–</td>\n",
    "      <td>one-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Is conversion rate different from 10%?</td>\n",
    "      <td>One-proportion z-test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>independent</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>normal</td>\n",
    "      <td>Do users who saw the new recommendation engine spend more time on site?</td>\n",
    "      <td>Two-sample t-test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>independent</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>non-normal</td>\n",
    "      <td>Is there a difference in revenue between users who got coupon A vs B?</td>\n",
    "      <td>Mann-Whitney U test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>paired</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>normal</td>\n",
    "      <td>Did users spend more after homepage redesign?</td>\n",
    "      <td>Paired t-test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>paired</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>non-normal</td>\n",
    "      <td>Did time on site change after layout update (skewed)?</td>\n",
    "      <td>Wilcoxon signed-rank test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>binary</td>\n",
    "      <td>independent</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Does a new CTA improve conversions?</td>\n",
    "      <td>Proportions z-test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>binary</td>\n",
    "      <td>paired</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Do users convert more after adding trust badges?</td>\n",
    "      <td>McNemar’s test</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>categorical</td>\n",
    "      <td>independent</td>\n",
    "      <td>multi-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Do plan choices differ between layout A/B/C?</td>\n",
    "      <td>Chi-square test</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Hypothesis Test Selection Matrix (Click to Expand)</strong></summary>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Outcome Type</th>\n",
    "      <th>Group Relationship</th>\n",
    "      <th>Group Count</th>\n",
    "      <th>Outcome Distribution</th>\n",
    "      <th>Business Problem</th>\n",
    "      <th>Recommended Test</th>\n",
    "      <th>Notes</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>–</td>\n",
    "      <td>one-sample</td>\n",
    "      <td>normal</td>\n",
    "      <td>Is average order value different from $50?</td>\n",
    "      <td>One-sample t-test</td>\n",
    "      <td>Use Wilcoxon signed-rank if not normal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>binary</td>\n",
    "      <td>–</td>\n",
    "      <td>one-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Is conversion rate different from 10%?</td>\n",
    "      <td>One-proportion z-test</td>\n",
    "      <td>Use binomial exact test if n is small</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>independent</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>normal</td>\n",
    "      <td>Do users who saw recs spend more time on site?</td>\n",
    "      <td>Two-sample t-test</td>\n",
    "      <td>Use Welch’s t-test if variances unequal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>independent</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>non-normal</td>\n",
    "      <td>Is revenue different between coupon A vs B?</td>\n",
    "      <td>Mann-Whitney U test</td>\n",
    "      <td>Non-parametric; tests medians</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>paired</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>normal</td>\n",
    "      <td>Did users spend more after redesign?</td>\n",
    "      <td>Paired t-test</td>\n",
    "      <td>Assumes differences are normal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>paired</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>non-normal</td>\n",
    "      <td>Did time on site change (skewed)?</td>\n",
    "      <td>Wilcoxon signed-rank test</td>\n",
    "      <td>Use for non-normal paired diffs</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>binary</td>\n",
    "      <td>independent</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Does new CTA improve conversion?</td>\n",
    "      <td>Proportions z-test</td>\n",
    "      <td>Chi-square for raw counts</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>binary</td>\n",
    "      <td>paired</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Do users convert more after badges?</td>\n",
    "      <td>McNemar’s test</td>\n",
    "      <td>Use for paired binary outcomes</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>categorical</td>\n",
    "      <td>independent</td>\n",
    "      <td>multi-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Do plan choices differ across layouts?</td>\n",
    "      <td>Chi-square test</td>\n",
    "      <td>Expected counts ≥5 in each cell</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>count</td>\n",
    "      <td>independent</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>Poisson</td>\n",
    "      <td>Do users add more items to cart?</td>\n",
    "      <td>Poisson / NB test</td>\n",
    "      <td>Use NB if overdispersion (variance > mean)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>independent</td>\n",
    "      <td>multi-sample</td>\n",
    "      <td>normal</td>\n",
    "      <td>Does time spent differ across A/B/C?</td>\n",
    "      <td>ANOVA</td>\n",
    "      <td>Welch ANOVA if variances differ</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>continuous</td>\n",
    "      <td>independent</td>\n",
    "      <td>multi-sample</td>\n",
    "      <td>non-normal</td>\n",
    "      <td>Does spend differ across segments?</td>\n",
    "      <td>Kruskal-Wallis test</td>\n",
    "      <td>Non-parametric alternative to ANOVA</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>any</td>\n",
    "      <td>any</td>\n",
    "      <td>any</td>\n",
    "      <td>–</td>\n",
    "      <td>Is effect still significant after adjusting for device & region?</td>\n",
    "      <td>Regression (linear / logistic)</td>\n",
    "      <td>Use when controlling for covariates</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>any</td>\n",
    "      <td>any</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>What’s the probability that B beats A?</td>\n",
    "      <td>Bayesian A/B test</td>\n",
    "      <td>Reports posterior probability instead of p-value</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>any</td>\n",
    "      <td>any</td>\n",
    "      <td>two-sample</td>\n",
    "      <td>–</td>\n",
    "      <td>Is observed lift statistically rare?</td>\n",
    "      <td>Permutation / Bootstrap</td>\n",
    "      <td>Use when assumptions are violated</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Pending Changes (Click to Expand)</strong></summary>\n",
    "\n",
    "Hard-Coding: Outcome text parametrize by variable\n",
    "\n",
    "Final text output vs Intermediate text output\n",
    "Verbosity\n",
    "\n",
    "Quizzed on what test to use\n",
    "Top hidden texts - Consolidation\n",
    "\n",
    "Visual summary\n",
    "\tFor two-group comparisons:\n",
    "\tShow boxplots or histograms.\n",
    "\tShow a confidence interval difference plot.\n",
    "\n",
    "Missing tree\n",
    "\n",
    "3 types of lines\n",
    "\n",
    "Validation layer\n",
    "\tCatch missing keys, unexpected values, or conflicting settings.\n",
    "\te.g., \"multi-sample\" with \"group_relationship\" == \"paired\" makes no sense.\n",
    "\n",
    "\n",
    "Real-world scenario hooks\n",
    "\tMaybe a dropdown or config print block with sample business questions:\n",
    "\t“Compare email click rates across campaigns”\n",
    "\t“Is the new homepage improving session duration?”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data-setup\"></a>\n",
    "# 🗂️ Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Settings\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "pretty_json = lambda d: display(HTML(f\"\"\"\n",
    "<pre style='font-size:14px; font-family:monospace;'>\n",
    "{json.dumps(d, indent=4)\n",
    "   .replace(\": null\", ': <span style=\"color:crimson;\"><b>null</b></span>')\n",
    "   .replace(': \"NA\"', ': <span style=\"color:crimson;\"><b>\"NA\"</b></span>')}\n",
    "</pre>\n",
    "<hr style='border: none; height: 1px; background-color: #ddd;' />\n",
    "\"\"\"))\n",
    "\n",
    "# Data Transformation Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "my_seed=1995\n",
    "\n",
    "# Stats Libraries\n",
    "from scipy.stats import (\n",
    "    ttest_1samp, ttest_rel, ttest_ind, wilcoxon, mannwhitneyu,\n",
    "    shapiro, chi2_contingency, f_oneway, kruskal, binom_test, fisher_exact, levene\n",
    ")\n",
    "from statsmodels.stats.proportion import proportions_ztest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generate-data\"></a>\n",
    "#### 🧪 Generate Data from Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_from_config(config, seed=my_seed):\n",
    "    \"\"\"\n",
    "    Generates synthetic data based on the specified hypothesis test configuration.\n",
    "\n",
    "    This function:\n",
    "    - Supports one-sample and two-sample tests for continuous and binary outcomes\n",
    "    - Simulates data using normal or binomial distributions depending on outcome type\n",
    "    - Applies treatment effect to simulate group differences\n",
    "    - Returns a DataFrame in the appropriate structure for the given test setup\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Dictionary specifying the test scenario with keys:\n",
    "        - 'outcome_type' (e.g., 'continuous', 'binary')\n",
    "        - 'group_count' ('one-sample' or 'two-sample')\n",
    "        - 'group_relationship' ('independent' or 'paired')\n",
    "        - 'sample_size': int (per group)\n",
    "        - 'effect_size': float (simulated difference to inject)\n",
    "        - 'population_mean': float (only used for one-sample tests)\n",
    "\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility (default = my_seed)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        A synthetic dataset compatible with the selected test type\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(my_seed)\n",
    "    \n",
    "    outcome = config['outcome_type']\n",
    "    group_count = config['group_count']\n",
    "    relationship = config['group_relationship']\n",
    "    size = config['sample_size']\n",
    "    effect = config['effect_size']\n",
    "    pop_mean = config.get('population_mean', 0)\n",
    "\n",
    "    # 1️⃣ One-sample case\n",
    "    if group_count == 'one-sample':\n",
    "        if outcome == 'continuous':\n",
    "            values = np.random.normal(loc=pop_mean + effect, scale=1.0, size=size)\n",
    "            df = pd.DataFrame({'value': values})\n",
    "        elif outcome == 'binary':\n",
    "            prob = pop_mean + effect\n",
    "            values = np.random.binomial(1, prob, size=size)\n",
    "            df = pd.DataFrame({'value': values})\n",
    "        else:\n",
    "            raise NotImplementedError(\"One-sample generation only supports continuous/binary for now.\")\n",
    "\n",
    "    # 2️⃣ Two-sample case\n",
    "    elif group_count == 'two-sample':\n",
    "        if relationship == 'independent':\n",
    "            if outcome == 'continuous':\n",
    "                A = np.random.normal(loc=5.0, scale=1.0, size=size)\n",
    "                B = np.random.normal(loc=5.0 + effect, scale=1.0, size=size)\n",
    "            elif outcome == 'binary':\n",
    "                A = np.random.binomial(1, 0.4, size=size)\n",
    "                B = np.random.binomial(1, 0.4 + effect, size=size)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            df = pd.DataFrame({\n",
    "                'group': ['A'] * size + ['B'] * size,\n",
    "                'value': np.concatenate([A, B])\n",
    "            })\n",
    "        \n",
    "        elif relationship == 'paired':\n",
    "            if outcome == 'continuous':\n",
    "                before = np.random.normal(loc=5.0, scale=1.0, size=size)\n",
    "                after = before + effect + np.random.normal(0, 0.5, size=size)\n",
    "            elif outcome == 'binary':\n",
    "                before = np.random.binomial(1, 0.4, size=size)\n",
    "                after = np.random.binomial(1, 0.4 + effect, size=size)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            df = pd.DataFrame({\n",
    "                'user_id': np.arange(size),\n",
    "                'group_A': before,\n",
    "                'group_B': after\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(\"Missing or invalid group relationship.\")\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Multi-sample not supported yet.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n",
    "\n",
    "<a id=\"define-test-config\"></a>\n",
    "#### ⚙️ Define Test Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<pre style='font-size:14px; font-family:monospace;'>\n",
       "{\n",
       "    \"outcome_type\": \"continuous\",\n",
       "    \"group_relationship\": \"independent\",\n",
       "    \"group_count\": \"two-sample\",\n",
       "    \"distribution\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"variance_equal\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"tail_type\": \"two-tailed\",\n",
       "    \"parametric\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"alpha\": 0.05,\n",
       "    \"sample_size\": 100,\n",
       "    \"effect_size\": 0.5\n",
       "}\n",
       "</pre>\n",
       "<hr style='border: none; height: 1px; background-color: #ddd;' />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    'outcome_type': 'continuous',        # continuous, binary, categorical, count\n",
    "    'group_relationship': 'independent', # independent or paired\n",
    "    'group_count': 'two-sample',         # one-sample, two-sample, multi-sample\n",
    "    'distribution': None,                # normal or non-normal → to be inferred\n",
    "    'variance_equal': None,              # equal or unequal → to be inferred\n",
    "    'tail_type': 'two-tailed',           # or 'one-tailed'\n",
    "    'parametric': None,                  # True or False → to be inferred\n",
    "    'alpha': 0.05,                       # significance level\n",
    "    'sample_size': 100,                  # per group\n",
    "    'effect_size': 0.5,                  # for generating synthetic difference\n",
    "}\n",
    "pretty_json(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>3.759367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>3.529421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>7.101191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>3.535178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>5.817922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>B</td>\n",
       "      <td>5.816507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>B</td>\n",
       "      <td>5.218070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>B</td>\n",
       "      <td>5.151804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>B</td>\n",
       "      <td>5.072379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>B</td>\n",
       "      <td>4.951768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    group     value\n",
       "0       A  3.759367\n",
       "1       A  3.529421\n",
       "2       A  7.101191\n",
       "3       A  3.535178\n",
       "4       A  5.817922\n",
       "..    ...       ...\n",
       "195     B  5.816507\n",
       "196     B  5.218070\n",
       "197     B  5.151804\n",
       "198     B  5.072379\n",
       "199     B  4.951768\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = generate_data_from_config(config)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test-setup\"></a>\n",
    "\n",
    "# 🛠️ Test Setup\n",
    "\n",
    "\n",
    "<details><summary><strong>📖 Test Settings Explanation (Click to Expand)</strong></summary>\n",
    "\n",
    "### 📊 **Test Type (test_type)**\n",
    "This setting defines the type of test you want to perform.\n",
    "\n",
    "- **one_sample**: Comparing the sample mean against a known value (e.g., a population mean).\n",
    "- **two_sample**: Comparing the means of two independent groups (e.g., A vs B).\n",
    "- **paired**: Comparing means from the same group at two different times (before vs after).\n",
    "- **proportions**: Comparing proportions (e.g., the conversion rates of two groups).\n",
    "\n",
    "**Example**: You might want to test if the mean age of two groups of people (Group A and Group B) differs, or if the proportion of people who converted in each group is different.\n",
    "\n",
    "### 📏 **Tail Type (tail_type)**\n",
    "This setting determines whether you are performing a one-tailed or two-tailed test.\n",
    "\n",
    "- **one_tailed**: You are testing if the value is greater than or less than the reference value (directional).\n",
    "- **two_tailed**: You are testing if the value is different from the reference value, either higher or lower (non-directional).\n",
    "\n",
    "**Example**:  \n",
    "- **One-tailed**: Testing if new treatment increases sales (you only care if it's greater).  \n",
    "- **Two-tailed**: Testing if there is any difference in sales between two treatments (it could be either an increase or decrease).\n",
    "\n",
    "### 🧮 **Parametric (parametric)**\n",
    "This setting indicates whether the test is **parametric** or **non-parametric**.\n",
    "\n",
    "- **True (Parametric)**: This means we assume that the data follows a certain distribution, often a **normal distribution**. The most common parametric tests are **t-tests** and **z-tests**. Parametric tests are generally more powerful if the assumptions are met.\n",
    "  \n",
    "- **False (Non-Parametric)**: Non-parametric tests don’t assume any specific distribution. These are used when the data doesn’t follow a normal distribution or when the sample size is small. Examples include **Mann-Whitney U** (alternative to the t-test) and **Wilcoxon Signed-Rank** (alternative to paired t-test).\n",
    "\n",
    "**Why does this matter?**  \n",
    "Parametric tests tend to be more powerful because they make assumptions about the distribution of the data (e.g., normality). Non-parametric tests are more flexible and can be used when these assumptions are not met, but they may be less powerful.\n",
    "\n",
    "### 📊 **Equal Variance (equal_variance)**\n",
    "This setting is used specifically for **two-sample t-tests**.\n",
    "\n",
    "- **True**: Assumes that the two groups have **equal variances** (i.e., the spread of data is the same in both groups). This is used for the **pooled t-test**.\n",
    "  \n",
    "- **False**: Assumes the two groups have **different variances**. This is used for the **Welch t-test**, which is more robust when the assumption of equal variances is violated.\n",
    "\n",
    "**Why is this important?**  \n",
    "If the variances are not equal, using a pooled t-test (which assumes equal variance) can lead to incorrect conclusions. The Welch t-test is safer when in doubt about the equality of variances.\n",
    "\n",
    "### 🔑 **Significance Level (alpha)**\n",
    "The **alpha** level is your **threshold for statistical significance**.\n",
    "\n",
    "- Commonly set at **0.05**, this means that you are willing to accept a 5% chance of wrongly rejecting the null hypothesis (i.e., a 5% chance of a Type I error).\n",
    "  \n",
    "- If the **p-value** (calculated from your test) is less than **alpha**, you reject the null hypothesis. If it's greater than alpha, you fail to reject the null hypothesis.\n",
    "\n",
    "**Example**:  \n",
    "- **alpha = 0.05** means there’s a 5% risk of concluding that a treatment has an effect when it actually doesn’t.\n",
    "\n",
    "### 🎯 **Putting It All Together**\n",
    "\n",
    "For instance, let's say you're testing if a new feature (Group A) increases user engagement compared to the existing feature (Group B). Here’s how each configuration works together:\n",
    "\n",
    "- **test_type** = `'two_sample'`: You're comparing two independent groups (A vs B).\n",
    "- **tail_type** = `'two_tailed'`: You’re testing if there’s any difference (increase or decrease) in engagement.\n",
    "- **parametric** = `True`: You assume the data is normally distributed, so a t-test will be appropriate.\n",
    "- **equal_variance** = `True`: You assume the two groups have equal variance, so you’ll use a pooled t-test.\n",
    "- **alpha** = `0.05`: You’re using a 5% significance level for your hypothesis test.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n",
    "\n",
    "<a id=\"validate-config\"></a>\n",
    "#### 🧪 Validate Configuration Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_config(config):\n",
    "    \"\"\"\n",
    "    Validates the hypothesis test configuration dictionary for completeness and logical consistency.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary to validate.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Raises ValueError if issues are found.\n",
    "    \"\"\"\n",
    "    \n",
    "    required_keys = [\n",
    "        'outcome_type', 'group_relationship', 'group_count', 'distribution',\n",
    "        'variance_equal', 'tail_type', 'parametric', 'alpha', 'sample_size', 'effect_size'\n",
    "    ]\n",
    "\n",
    "    valid_outcome_types = ['continuous', 'binary', 'categorical', 'count']\n",
    "    valid_group_relationships = ['independent', 'paired']\n",
    "    valid_group_counts = ['one-sample', 'two-sample', 'multi-sample']\n",
    "    valid_distributions = ['normal', 'non-normal', 'NA', None]\n",
    "    valid_variance_flags = ['equal', 'unequal', 'NA', None]\n",
    "    valid_tail_types = ['one-tailed', 'two-tailed']\n",
    "    valid_parametric_flags = [True, False, 'NA', None]\n",
    "\n",
    "    # 1. Missing keys\n",
    "    for key in required_keys:\n",
    "        if key not in config:\n",
    "            raise ValueError(f\"❌ Missing key in config: '{key}'\")\n",
    "\n",
    "    # 2. Check values are within known sets\n",
    "    if config['outcome_type'] not in valid_outcome_types:\n",
    "        raise ValueError(f\"❌ Invalid outcome_type: {config['outcome_type']}\")\n",
    "\n",
    "    if config['group_relationship'] not in valid_group_relationships and config['group_count'] != 'one-sample':\n",
    "        raise ValueError(f\"❌ Invalid group_relationship: {config['group_relationship']}\")\n",
    "\n",
    "    if config['group_count'] not in valid_group_counts:\n",
    "        raise ValueError(f\"❌ Invalid group_count: {config['group_count']}\")\n",
    "\n",
    "    if config['distribution'] not in valid_distributions:\n",
    "        raise ValueError(f\"❌ Invalid distribution: {config['distribution']}\")\n",
    "\n",
    "    if config['variance_equal'] not in valid_variance_flags:\n",
    "        raise ValueError(f\"❌ Invalid variance_equal: {config['variance_equal']}\")\n",
    "\n",
    "    if config['tail_type'] not in valid_tail_types:\n",
    "        raise ValueError(f\"❌ Invalid tail_type: {config['tail_type']}\")\n",
    "\n",
    "    if config['parametric'] not in valid_parametric_flags:\n",
    "        raise ValueError(f\"❌ Invalid parametric flag: {config['parametric']}\")\n",
    "\n",
    "    if not (0 < config['alpha'] < 1):\n",
    "        raise ValueError(\"❌ Alpha level should be between 0 and 1.\")\n",
    "\n",
    "    if config['sample_size'] <= 0:\n",
    "        raise ValueError(\"❌ Sample size must be positive.\")\n",
    "\n",
    "    # 3. Logical combination checks\n",
    "    # One-sample + non-independent → override to independent\n",
    "    if config['group_count'] == 'one-sample' and config['group_relationship'] != 'independent':\n",
    "        print(\"⚠️ Overriding group_relationship to 'independent' for one-sample test.\")\n",
    "        config['group_relationship'] = 'independent'\n",
    "\n",
    "    # Multi-sample + paired → not supported by this module\n",
    "    if config['group_count'] == 'multi-sample' and config['group_relationship'] == 'paired':\n",
    "        raise ValueError(\"❌ Paired relationship not supported for multi-sample tests.\")\n",
    "\n",
    "    # One-sample + missing population_mean → invalid config\n",
    "    if config['group_count'] == 'one-sample' and 'population_mean' not in config:\n",
    "        raise ValueError(\"❌ One-sample tests require `population_mean` to be specified.\")\n",
    "    \n",
    "    # Paired + categorical (not supported by this module)\n",
    "    if config['outcome_type'] == 'categorical' and config['group_relationship'] == 'paired':\n",
    "        raise ValueError(\"❌ Paired tests are not supported for categorical outcomes in this module.\")\n",
    "\n",
    "    # Binary outcome + parametric + small n → warn about z-test validity\n",
    "    if config['outcome_type'] == 'binary' and config['parametric'] is True:\n",
    "        if config['sample_size'] < 30:\n",
    "            print(\"⚠️ Sample size < 30 → z-test assumptions (np > 5) may be violated. Consider Fisher’s Exact.\")\n",
    "\n",
    "    # Parametric test selected, but distribution is missing\n",
    "    if config['parametric'] is True and config['distribution'] in ['NA', None]:\n",
    "        raise ValueError(\"❌ Parametric test requested, but distribution is not confirmed as normal.\")\n",
    "\n",
    "    # Count outcome + one-sample → not supported\n",
    "    if config['outcome_type'] == 'count' and config['group_count'] == 'one-sample':\n",
    "        raise ValueError(\"🔒 One-sample tests for count data are not supported by this module.\")\n",
    "\n",
    "    # Effect size unusually large or small (soft validation)\n",
    "    if config['effect_size'] < 0 or config['effect_size'] > 2:\n",
    "        print(\"⚠️ Effect size is unusually extreme. Are you simulating a realistic scenario?\")\n",
    "\n",
    "    # Optional: variance check mismatch\n",
    "    if config['variance_equal'] not in ['equal', 'unequal', 'NA', None]:\n",
    "        raise ValueError(f\"❌ Invalid variance_equal flag: {config['variance_equal']}\")\n",
    "\n",
    "    # Optional: group relationship irrelevant in one-sample, but present\n",
    "    if config['group_count'] == 'one-sample' and config.get('group_relationship') != 'independent':\n",
    "        print(\"⚠️ One-sample tests don’t require `group_relationship`. Defaulting to 'independent'.\")\n",
    "        config['group_relationship'] = 'independent'\n",
    "\n",
    "\n",
    "    print(\"✅ Config validated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config validated successfully.\n"
     ]
    }
   ],
   "source": [
    "validate_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n",
    "\n",
    "<a id=\"print-config\"></a>\n",
    "#### 📋 Print Config Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_config_summary(config):\n",
    "    \"\"\"\n",
    "    Displays a structured summary of the test configuration with visual cues for missing or inferred values.\n",
    "\n",
    "    This function:\n",
    "    - Prints each key in the config with aligned formatting\n",
    "    - Highlights `None` or 'NA' values in red (terminal only)\n",
    "    - Provides a short inference summary based on group count and relationship\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing test settings like outcome type, group relationship,\n",
    "        distribution, variance assumption, parametric flag, and alpha level\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Prints the formatted configuration summary directly to output\n",
    "    \"\"\"\n",
    "\n",
    "    def highlight(value):\n",
    "        if value in [None, 'NA'] or (isinstance(value, float) and np.isnan(value)):\n",
    "            return \"\\033[91mNone\\033[0m\"  # Red in terminal\n",
    "        return value\n",
    "\n",
    "    print(\"📋 Hypothesis Test Configuration Summary\\n\")\n",
    "\n",
    "    print(f\"🔸 Outcome Type            : {highlight(config['outcome_type'])}\")\n",
    "    print(f\"🔸 Group Relationship      : {highlight(config['group_relationship'])}\")\n",
    "    print(f\"🔸 Group Count             : {highlight(config['group_count'])}\")\n",
    "    print(f\"🔸 Distribution of Outcome : {highlight(config['distribution'])}\")\n",
    "    print(f\"🔸 Equal Variance          : {highlight(config['variance_equal'])}\")\n",
    "    print(f\"🔸 Parametric Test         : {highlight(config['parametric'])}\")\n",
    "    print(f\"🔸 Tail Type               : {highlight(config['tail_type'])}\")\n",
    "    print(f\"🔸 Significance Level α    : {highlight(config['alpha'])}\")\n",
    "\n",
    "    print(\"\\n🧠 Inference Summary:\")\n",
    "    if config['group_count'] == 'one-sample':\n",
    "        print(\"→ This is a one-sample test comparing a sample to a known value.\")\n",
    "    elif config['group_count'] == 'two-sample':\n",
    "        if config['group_relationship'] == 'independent':\n",
    "            print(\"→ Comparing two independent groups (A vs B).\")\n",
    "        elif config['group_relationship'] == 'paired':\n",
    "            print(\"→ Comparing paired measurements (before vs after, same users).\")\n",
    "    display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<pre style='font-size:14px; font-family:monospace;'>\n",
       "{\n",
       "    \"outcome_type\": \"continuous\",\n",
       "    \"group_relationship\": \"independent\",\n",
       "    \"group_count\": \"two-sample\",\n",
       "    \"distribution\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"variance_equal\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"tail_type\": \"two-tailed\",\n",
       "    \"parametric\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"alpha\": 0.05,\n",
       "    \"sample_size\": 100,\n",
       "    \"effect_size\": 0.5\n",
       "}\n",
       "</pre>\n",
       "<hr style='border: none; height: 1px; background-color: #ddd;' />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Hypothesis Test Configuration Summary\n",
      "\n",
      "🔸 Outcome Type            : continuous\n",
      "🔸 Group Relationship      : independent\n",
      "🔸 Group Count             : two-sample\n",
      "🔸 Distribution of Outcome : \u001b[91mNone\u001b[0m\n",
      "🔸 Equal Variance          : \u001b[91mNone\u001b[0m\n",
      "🔸 Parametric Test         : \u001b[91mNone\u001b[0m\n",
      "🔸 Tail Type               : two-tailed\n",
      "🔸 Significance Level α    : 0.05\n",
      "\n",
      "🧠 Inference Summary:\n",
      "→ Comparing two independent groups (A vs B).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretty_json(config)\n",
    "print_config_summary(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"inference\"></a>\n",
    "\n",
    "# 📈 Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"infer-distribution\"></a>\n",
    "#### 🔍 Infer Distribution\n",
    "\n",
    "<details><summary><strong>📖 Normality Check (Click to Expand)</strong></summary>\n",
    "\n",
    "#### 📘 Why Are We Checking for Normality?\n",
    "\n",
    "Many hypothesis tests (like the **t-test**) rely on the assumption that the **outcome variable is normally distributed**.  \n",
    "This is particularly important when working with **continuous** outcome variables in small to moderate-sized samples.\n",
    "\n",
    "#### 🧪 Test Used: **Shapiro-Wilk**\n",
    "\n",
    "The **Shapiro-Wilk test** checks whether the sample comes from a normal distribution.\n",
    "\n",
    "- **Null Hypothesis (H₀)**: The data follows a normal distribution  \n",
    "- **Alternative Hypothesis (H₁)**: The data does not follow a normal distribution\n",
    "\n",
    "#### 🧠 Interpretation:\n",
    "\n",
    "- **p > 0.05** → Fail to reject H₀ → The data is **likely a normal distribution** ✅  \n",
    "- **p < 0.05** → Reject H₀ → The data is **likely a non-normal distribution** ⚠️\n",
    "\n",
    "We check this **per group**, and only if the outcome variable is **continuous**.\n",
    "\n",
    "#### ❗Note:\n",
    "\n",
    "- No need to check normality for **binary**, **categorical**, or **count** data.\n",
    "- For **paired tests**, we assess normality on the **differences** between paired observations.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_distribution_from_data(config, df):\n",
    "    \"\"\"\n",
    "    Infers whether the outcome variable follows a normal distribution using the Shapiro-Wilk test.\n",
    "\n",
    "    This function:\n",
    "    - Checks if the outcome type is continuous (required for normality testing)\n",
    "    - Applies Shapiro-Wilk test to one or both groups depending on group structure\n",
    "    - Updates the 'distribution' key in the config as 'normal', 'non-normal', or 'NA'\n",
    "    - Logs interpretation and decision in a reader-friendly format\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing 'outcome_type', 'group_count', and 'group_relationship'\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing outcome values and group assignments\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Updated config dictionary with the 'distribution' key set\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n🔍 Step: Infer Distribution of Outcome Variable\")\n",
    "\n",
    "    group_count = config['group_count']\n",
    "    relationship = config['group_relationship']\n",
    "    outcome = config['outcome_type']\n",
    "\n",
    "    if outcome != 'continuous':\n",
    "        print(f\"⚠️ Skipping: Outcome type = `{outcome}` → normality check not applicable.\")\n",
    "        config['distribution'] = 'NA'\n",
    "        display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "        return config\n",
    "\n",
    "    print(\"📘 Checking if the outcome variable follows a normal distribution\")\n",
    "    print(\"   Using Shapiro-Wilk Test\")\n",
    "    print(\"   H₀: Data comes from a normal distribution\")\n",
    "    print(\"   H₁: Data does NOT come from a normal distribution\\n\")\n",
    "\n",
    "    if group_count == 'one-sample':\n",
    "        print(\"• One-sample case → testing entire column\")\n",
    "        stat, p = shapiro(df['value'])\n",
    "        print(f\"• Shapiro-Wilk p-value = {p:.4f}\")\n",
    "\n",
    "        if p > 0.05:\n",
    "            print(\"✅ Fail to reject H₀ → Data is likely a normal distribution\")\n",
    "            config['distribution'] = 'normal'\n",
    "        else:\n",
    "            print(\"⚠️ Reject H₀ → Data is likely a non-normal distribution\")\n",
    "            config['distribution'] = 'non-normal'\n",
    "\n",
    "        print(f\"📦 Final Decision → config['distribution'] = `{config['distribution']}`\")\n",
    "        display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "        return config\n",
    "\n",
    "    elif group_count == 'two-sample':\n",
    "        print(f\"• Two-sample ({relationship}) case → testing both groups\")\n",
    "\n",
    "        if relationship == 'independent':\n",
    "            a = df[df['group'] == 'A']['value']\n",
    "            b = df[df['group'] == 'B']['value']\n",
    "        elif relationship == 'paired':\n",
    "            a = df['group_A']\n",
    "            b = df['group_B']\n",
    "        else:\n",
    "            print(\"❌ Invalid group relationship\")\n",
    "            config['distribution'] = 'NA'\n",
    "            display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "            return config\n",
    "\n",
    "        p1 = shapiro(a).pvalue\n",
    "        p2 = shapiro(b).pvalue\n",
    "\n",
    "        print(f\"• Group A → Shapiro-Wilk p = {p1:.4f} →\", \n",
    "              \"Fail to reject H₀ ✅ (likely a normal distribution)\" if p1 > 0.05 \n",
    "              else \"Reject H₀ ⚠️ (likely a non-normal distribution)\")\n",
    "\n",
    "        print(f\"• Group B → Shapiro-Wilk p = {p2:.4f} →\", \n",
    "              \"Fail to reject H₀ ✅ (likely a normal distribution)\" if p2 > 0.05 \n",
    "              else \"Reject H₀ ⚠️ (likely a non-normal distribution)\")\n",
    "\n",
    "        if p1 > 0.05 and p2 > 0.05:\n",
    "            print(\"✅ Both groups are likely drawn from normal distributions\")\n",
    "            config['distribution'] = 'normal'\n",
    "        else:\n",
    "            print(\"⚠️ At least one group does not appear normally distributed\")\n",
    "            config['distribution'] = 'non-normal'\n",
    "\n",
    "        print(f\"📦 Final Decision → config['distribution'] = `{config['distribution']}`\")\n",
    "        display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "        return config\n",
    "\n",
    "    else:\n",
    "        print(\"❌ Unsupported group count for distribution check.\")\n",
    "        config['distribution'] = 'NA'\n",
    "        display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Step: Infer Distribution of Outcome Variable\n",
      "📘 Checking if the outcome variable follows a normal distribution\n",
      "   Using Shapiro-Wilk Test\n",
      "   H₀: Data comes from a normal distribution\n",
      "   H₁: Data does NOT come from a normal distribution\n",
      "\n",
      "• Two-sample (independent) case → testing both groups\n",
      "• Group A → Shapiro-Wilk p = 0.4534 → Fail to reject H₀ ✅ (likely a normal distribution)\n",
      "• Group B → Shapiro-Wilk p = 0.7145 → Fail to reject H₀ ✅ (likely a normal distribution)\n",
      "✅ Both groups are likely drawn from normal distributions\n",
      "📦 Final Decision → config['distribution'] = `normal`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<pre style='font-size:14px; font-family:monospace;'>\n",
       "{\n",
       "    \"outcome_type\": \"continuous\",\n",
       "    \"group_relationship\": \"independent\",\n",
       "    \"group_count\": \"two-sample\",\n",
       "    \"distribution\": \"normal\",\n",
       "    \"variance_equal\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"tail_type\": \"two-tailed\",\n",
       "    \"parametric\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"alpha\": 0.05,\n",
       "    \"sample_size\": 100,\n",
       "    \"effect_size\": 0.5\n",
       "}\n",
       "</pre>\n",
       "<hr style='border: none; height: 1px; background-color: #ddd;' />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Hypothesis Test Configuration Summary\n",
      "\n",
      "🔸 Outcome Type            : continuous\n",
      "🔸 Group Relationship      : independent\n",
      "🔸 Group Count             : two-sample\n",
      "🔸 Distribution of Outcome : normal\n",
      "🔸 Equal Variance          : \u001b[91mNone\u001b[0m\n",
      "🔸 Parametric Test         : \u001b[91mNone\u001b[0m\n",
      "🔸 Tail Type               : two-tailed\n",
      "🔸 Significance Level α    : 0.05\n",
      "\n",
      "🧠 Inference Summary:\n",
      "→ Comparing two independent groups (A vs B).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = infer_distribution_from_data(config, df)\n",
    "pretty_json(config)\n",
    "print_config_summary(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n",
    "\n",
    "<a id=\"infer-variance\"></a>\n",
    "#### 📏 Infer Variance\n",
    "\n",
    "<details><summary><strong>📖 Equal Variance Check (Click to Expand)</strong></summary>\n",
    "\n",
    "#### 📘 Why Are We Checking for Equal Variance?\n",
    "\n",
    "When comparing **two independent groups** using a **parametric test** (like a two-sample t-test),  \n",
    "we assume that both groups have **equal variances** — this is called the **homogeneity of variance** assumption.\n",
    "\n",
    "Failing to meet this assumption can lead to incorrect conclusions if the wrong test is applied.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧪 Test Used: **Levene’s Test**\n",
    "\n",
    "Levene’s Test checks whether the **spread (variance)** of values is roughly the same in both groups.\n",
    "\n",
    "- **Null Hypothesis (H₀)**: Variance in Group A = Variance in Group B  \n",
    "- **Alternative Hypothesis (H₁)**: Variances are different between the groups\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧠 Interpretation:\n",
    "\n",
    "- **p > 0.05** → Fail to reject H₀ → Variances are **likely equal** ✅  \n",
    "- **p < 0.05** → Reject H₀ → Variances are **likely unequal** ⚠️\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ When Should You Check This?\n",
    "\n",
    "- ✔️ Check **only** when:\n",
    "  - You’re comparing **two groups**\n",
    "  - The groups are **independent**\n",
    "  - The outcome is **continuous**\n",
    "\n",
    "- ❌ Don’t check if:\n",
    "  - The test is **one-sample**\n",
    "  - The groups are **paired** (since variance of differences is what matters)\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_variance_equality(config, df):\n",
    "    \"\"\"\n",
    "    Infers whether the variances across two independent groups are equal using Levene's test.\n",
    "\n",
    "    This function:\n",
    "    - Checks if the variance assumption is relevant based on config\n",
    "    - Runs Levene’s test to compare the variances of Group A and Group B\n",
    "    - Updates the 'variance_equal' key in the config as 'equal', 'unequal', or 'NA'\n",
    "    - Logs interpretation of the test result\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing 'group_count' and 'group_relationship'\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing 'group' and 'value' columns\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Updated config dictionary with the 'variance_equal' key set\n",
    "    \"\"\"\n",
    "    print(\"\\n📏 **Step: Infer Equality of Variance Across Groups**\")\n",
    "\n",
    "    # Skip if not applicable\n",
    "    if config['group_count'] != 'two-sample' or config['group_relationship'] != 'independent':\n",
    "        print(\"⚠️ Skipping variance check: Only applicable for two-sample independent tests.\")\n",
    "        config['variance_equal'] = 'NA'\n",
    "        return config\n",
    "\n",
    "    print(\"📘 We're checking if the spread (variance) of the outcome variable is similar across groups A and B.\")\n",
    "    print(\"   This is important for choosing between a **pooled t-test** vs **Welch’s t-test**.\")\n",
    "    print(\"🔬 Test Used: Levene’s Test for Equal Variance\")\n",
    "    print(\"   H₀: Variance in Group A = Variance in Group B\")\n",
    "    print(\"   H₁: Variances are different\")\n",
    "\n",
    "    # Extract data\n",
    "    a = df[df['group'] == 'A']['value']\n",
    "    b = df[df['group'] == 'B']['value']\n",
    "\n",
    "    # Run Levene's test\n",
    "    stat, p = levene(a, b)\n",
    "    print(f\"\\n📊 Levene’s Test Result:\")\n",
    "    print(f\"• Test Statistic = {stat:.4f}\")\n",
    "    print(f\"• p-value        = {p:.4f}\")\n",
    "\n",
    "    if p > 0.05:\n",
    "        print(\"✅ Fail to reject H₀ → Variances appear equal across groups\")\n",
    "        config['variance_equal'] = 'equal'\n",
    "    else:\n",
    "        print(\"⚠️ Reject H₀ → Variances appear unequal\")\n",
    "        config['variance_equal'] = 'unequal'\n",
    "\n",
    "    print(f\"\\n📦 Final Decision → config['variance_equal'] = `{config['variance_equal']}`\")\n",
    "    display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 **Step: Infer Equality of Variance Across Groups**\n",
      "📘 We're checking if the spread (variance) of the outcome variable is similar across groups A and B.\n",
      "   This is important for choosing between a **pooled t-test** vs **Welch’s t-test**.\n",
      "🔬 Test Used: Levene’s Test for Equal Variance\n",
      "   H₀: Variance in Group A = Variance in Group B\n",
      "   H₁: Variances are different\n",
      "\n",
      "📊 Levene’s Test Result:\n",
      "• Test Statistic = 1.0751\n",
      "• p-value        = 0.3011\n",
      "✅ Fail to reject H₀ → Variances appear equal across groups\n",
      "\n",
      "📦 Final Decision → config['variance_equal'] = `equal`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<pre style='font-size:14px; font-family:monospace;'>\n",
       "{\n",
       "    \"outcome_type\": \"continuous\",\n",
       "    \"group_relationship\": \"independent\",\n",
       "    \"group_count\": \"two-sample\",\n",
       "    \"distribution\": \"normal\",\n",
       "    \"variance_equal\": \"equal\",\n",
       "    \"tail_type\": \"two-tailed\",\n",
       "    \"parametric\": <span style=\"color:crimson;\"><b>null</b></span>,\n",
       "    \"alpha\": 0.05,\n",
       "    \"sample_size\": 100,\n",
       "    \"effect_size\": 0.5\n",
       "}\n",
       "</pre>\n",
       "<hr style='border: none; height: 1px; background-color: #ddd;' />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Hypothesis Test Configuration Summary\n",
      "\n",
      "🔸 Outcome Type            : continuous\n",
      "🔸 Group Relationship      : independent\n",
      "🔸 Group Count             : two-sample\n",
      "🔸 Distribution of Outcome : normal\n",
      "🔸 Equal Variance          : equal\n",
      "🔸 Parametric Test         : \u001b[91mNone\u001b[0m\n",
      "🔸 Tail Type               : two-tailed\n",
      "🔸 Significance Level α    : 0.05\n",
      "\n",
      "🧠 Inference Summary:\n",
      "→ Comparing two independent groups (A vs B).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = infer_variance_equality(config, df)\n",
    "pretty_json(config)\n",
    "print_config_summary(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n",
    "\n",
    "<a id=\"infer-parametric\"></a>\n",
    "#### 📏 Infer Parametric Flag\n",
    "\n",
    "<details><summary><strong>📖 Parametric vs Non-Parametric (Click to Expand)</strong></summary>\n",
    "\n",
    "### 📘 What Does \"Parametric\" Mean?\n",
    "\n",
    "A **parametric test** assumes that the data follows a known distribution — typically a **normal distribution**.\n",
    "\n",
    "These tests also often assume:\n",
    "- Equal variances between groups (for two-sample cases)\n",
    "- Independent samples\n",
    "\n",
    "When those assumptions are met, parametric tests are **more powerful** (i.e., they detect real effects more easily).\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 What Happens If Assumptions Don’t Hold?\n",
    "\n",
    "You should use a **non-parametric test** — these don’t rely on strong distributional assumptions and are more robust, especially for small sample sizes or skewed data.\n",
    "\n",
    "Examples:\n",
    "| Parametric Test          | Non-Parametric Alternative       |\n",
    "|--------------------------|----------------------------------|\n",
    "| Two-sample t-test        | Mann-Whitney U test              |\n",
    "| Paired t-test            | Wilcoxon Signed-Rank test        |\n",
    "| ANOVA                    | Kruskal-Wallis test              |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 How We Decide Here\n",
    "\n",
    "In our pipeline, a test is **parametric** only if:\n",
    "\n",
    "- The outcome variable is **continuous**\n",
    "- The data is **normally distributed**\n",
    "- The variance is **equal**, if applicable (or marked `\"NA\"` for paired designs)\n",
    "\n",
    "If these aren’t all true, we default to a non-parametric test.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_parametric_flag(config):\n",
    "    \"\"\"\n",
    "    Infers whether a parametric or non-parametric test should be used based on the config.\n",
    "\n",
    "    This function:\n",
    "    - Applies logic based on distribution and variance assumptions\n",
    "    - Sets the 'parametric' key in the config to True, False, or 'NA'\n",
    "    - Prints reasoning for transparency and learning\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing 'outcome_type', 'distribution', and 'variance_equal'\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Updated config dictionary with the 'parametric' key populated\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n📏 Step: Decide Between Parametric vs Non-Parametric Approach\")\n",
    "\n",
    "    if config['outcome_type'] != 'continuous':\n",
    "        print(f\"⚠️ Skipping: Outcome type = `{config['outcome_type']}` → Parametric logic not applicable.\")\n",
    "        config['parametric'] = 'NA'\n",
    "        return config\n",
    "\n",
    "    is_normal = config['distribution'] == 'normal'\n",
    "    is_equal_var = config['variance_equal'] in ['equal', 'NA']  # NA = not required for paired\n",
    "\n",
    "    print(f\"🔍 Outcome type             = `{config['outcome_type']}`\")\n",
    "    print(f\"🔍 Distribution of outcome  = `{config['distribution']}`\")\n",
    "    print(f\"🔍 Variance equal flag      = `{config['variance_equal']}`\")\n",
    "\n",
    "    if is_normal and is_equal_var:\n",
    "        print(\"✅ Conditions met → Proceeding with a parametric test (e.g., t-test)\")\n",
    "        config['parametric'] = True\n",
    "    else:\n",
    "        print(\"⚠️ One or more assumptions violated → Using non-parametric alternative\")\n",
    "        config['parametric'] = False\n",
    "\n",
    "    print(f\"\\n📦 Final Decision → config['parametric'] = `{config['parametric']}`\")\n",
    "    display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📏 Step: Decide Between Parametric vs Non-Parametric Approach\n",
      "🔍 Outcome type             = `continuous`\n",
      "🔍 Distribution of outcome  = `normal`\n",
      "🔍 Variance equal flag      = `equal`\n",
      "✅ Conditions met → Proceeding with a parametric test (e.g., t-test)\n",
      "\n",
      "📦 Final Decision → config['parametric'] = `True`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<pre style='font-size:14px; font-family:monospace;'>\n",
       "{\n",
       "    \"outcome_type\": \"continuous\",\n",
       "    \"group_relationship\": \"independent\",\n",
       "    \"group_count\": \"two-sample\",\n",
       "    \"distribution\": \"normal\",\n",
       "    \"variance_equal\": \"equal\",\n",
       "    \"tail_type\": \"two-tailed\",\n",
       "    \"parametric\": true,\n",
       "    \"alpha\": 0.05,\n",
       "    \"sample_size\": 100,\n",
       "    \"effect_size\": 0.5\n",
       "}\n",
       "</pre>\n",
       "<hr style='border: none; height: 1px; background-color: #ddd;' />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Hypothesis Test Configuration Summary\n",
      "\n",
      "🔸 Outcome Type            : continuous\n",
      "🔸 Group Relationship      : independent\n",
      "🔸 Group Count             : two-sample\n",
      "🔸 Distribution of Outcome : normal\n",
      "🔸 Equal Variance          : equal\n",
      "🔸 Parametric Test         : True\n",
      "🔸 Tail Type               : two-tailed\n",
      "🔸 Significance Level α    : 0.05\n",
      "\n",
      "🧠 Inference Summary:\n",
      "→ Comparing two independent groups (A vs B).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = infer_parametric_flag(config)\n",
    "pretty_json(config)\n",
    "print_config_summary(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hypothesis-testing\"></a>\n",
    "<h1>🧪 Hypothesis Testing</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"determine-test\"></a>\n",
    "#### 🧭 Determine Test\n",
    "\n",
    "<details><summary><strong>📖 How We Select the Right Statistical Test (Click to Expand)</strong></summary>\n",
    "\n",
    "#### 🧠 What Are We Doing Here?\n",
    "\n",
    "Based on all the configuration values you’ve either set or inferred (`outcome_type`, `group_relationship`, `distribution`, etc),  \n",
    "we determine **which statistical test is most appropriate** for your hypothesis.\n",
    "\n",
    "This is the decision engine of the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "#### ⚙️ How the Logic Works\n",
    "\n",
    "We go through structured rules based on:\n",
    "\n",
    "| Config Field           | What it Affects                     |\n",
    "|------------------------|-------------------------------------|\n",
    "| `outcome_type`         | Binary / continuous / categorical   |\n",
    "| `group_count`          | One-sample / two-sample / multi     |\n",
    "| `group_relationship`   | Independent or paired               |\n",
    "| `distribution`         | Normal or non-normal                |\n",
    "| `variance_equal`       | Determines pooled vs Welch’s t-test |\n",
    "| `parametric`           | Whether to use parametric approach  |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧪 Example Mappings:\n",
    "\n",
    "| Scenario                                             | Selected Test               |\n",
    "|------------------------------------------------------|-----------------------------|\n",
    "| Continuous, 2 groups, normal, equal variance         | Two-sample t-test (pooled)  |\n",
    "| Continuous, 2 groups, non-normal                     | Mann-Whitney U              |\n",
    "| Binary, 2 groups, independent                        | Proportions z-test          |\n",
    "| Continuous, paired, non-normal                      | Wilcoxon Signed-Rank        |\n",
    "| 3+ groups, categorical outcome                       | Chi-square test             |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🧯 What Happens if It Can’t Decide?\n",
    "\n",
    "The function returns:  \n",
    "```python\n",
    "'test_not_found'\n",
    "This acts as a signal to handle rare or unsupported config combinations.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧭 Step: Determine Which Statistical Test to Use\n",
      "📦 Inputs:\n",
      "• Outcome Type         = `continuous`\n",
      "• Group Count          = `two-sample`\n",
      "• Group Relationship   = `independent`\n",
      "• Distribution         = `normal`\n",
      "• Equal Variance       = `equal`\n",
      "• Parametric Flag      = `True`\n",
      "\n",
      "🔍 Matching against known test cases...\n",
      "\n",
      "✅ Selected Test: `two_sample_ttest_pooled`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'two_sample_ttest_pooled'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def determine_test_to_run(config):\n",
    "    \"\"\"\n",
    "    Determines the appropriate statistical test based on the provided configuration.\n",
    "\n",
    "    This function:\n",
    "    - Maps outcome type, group count, group relationship, distribution, and parametric flags\n",
    "    to the correct hypothesis test\n",
    "    - Prints the reasoning and selected test\n",
    "    - Returns a string identifier for the test to be used\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        A dictionary containing keys like 'outcome_type', 'group_count', 'group_relationship',\n",
    "        'distribution', 'variance_equal', and 'parametric'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        A string representing the selected test name (e.g., 'two_sample_ttest_welch', 'mcnemar', etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n🧭 Step: Determine Which Statistical Test to Use\")\n",
    "    \n",
    "    outcome = config['outcome_type']\n",
    "    group_rel = config['group_relationship']\n",
    "    group_count = config['group_count']\n",
    "    dist = config['distribution']\n",
    "    equal_var = config['variance_equal']\n",
    "    parametric = config['parametric']\n",
    "\n",
    "    print(\"📦 Inputs:\")\n",
    "    print(f\"• Outcome Type         = `{outcome}`\")\n",
    "    print(f\"• Group Count          = `{group_count}`\")\n",
    "    print(f\"• Group Relationship   = `{group_rel}`\")\n",
    "    print(f\"• Distribution         = `{dist}`\")\n",
    "    print(f\"• Equal Variance       = `{equal_var}`\")\n",
    "    print(f\"• Parametric Flag      = `{parametric}`\")\n",
    "\n",
    "    print(\"\\n🔍 Matching against known test cases...\")\n",
    "\n",
    "    # One-sample\n",
    "    if group_count == 'one-sample':\n",
    "        if outcome == 'continuous':\n",
    "            test = 'one_sample_ttest' if dist == 'normal' else 'one_sample_wilcoxon'\n",
    "        elif outcome == 'binary':\n",
    "            test = 'one_proportion_ztest'\n",
    "        else:\n",
    "            test = 'test_not_found'\n",
    "\n",
    "    # Two-sample independent\n",
    "    elif group_count == 'two-sample' and group_rel == 'independent':\n",
    "        if outcome == 'continuous':\n",
    "            if parametric:\n",
    "                test = 'two_sample_ttest_pooled' if equal_var == 'equal' else 'two_sample_ttest_welch'\n",
    "            else:\n",
    "                test = 'mann_whitney_u'\n",
    "        elif outcome == 'binary':\n",
    "            test = 'two_proportion_ztest'\n",
    "        elif outcome == 'categorical':\n",
    "            test = 'chi_square'\n",
    "        else:\n",
    "            test = 'test_not_found'\n",
    "\n",
    "    # Two-sample paired\n",
    "    elif group_count == 'two-sample' and group_rel == 'paired':\n",
    "        if outcome == 'continuous':\n",
    "            test = 'paired_ttest' if parametric else 'wilcoxon_signed_rank'\n",
    "        elif outcome == 'binary':\n",
    "            test = 'mcnemar'\n",
    "        else:\n",
    "            test = 'test_not_found'\n",
    "\n",
    "    # Multi-group\n",
    "    elif group_count == 'multi-sample':\n",
    "        if outcome == 'continuous':\n",
    "            test = 'anova' if dist == 'normal' else 'kruskal_wallis'\n",
    "        elif outcome == 'categorical':\n",
    "            test = 'chi_square'\n",
    "        else:\n",
    "            test = 'test_not_found'\n",
    "\n",
    "    # Count data\n",
    "    elif outcome == 'count':\n",
    "        test = 'poisson_test'\n",
    "\n",
    "    else:\n",
    "        test = 'test_not_found'\n",
    "\n",
    "    print(f\"\\n✅ Selected Test: `{test}`\")\n",
    "    display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "    return test\n",
    "determine_test_to_run(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n",
    "\n",
    "<a id=\"print-hypothesis\"></a>\n",
    "#### 🧠 Print Hypothesis\n",
    "\n",
    "<details><summary><strong>📖 Hypothesis Structure & Interpretation (Click to Expand)</strong></summary>\n",
    "\n",
    "### 📘 What Are We Doing Here?\n",
    "\n",
    "This step generates a **formal hypothesis statement** for the selected test.\n",
    "\n",
    "Every statistical test is built around two competing ideas:\n",
    "\n",
    "- **H₀ (Null Hypothesis)** → The “status quo”. There is **no effect**, **no difference**, or **no association**.\n",
    "- **H₁ (Alternative Hypothesis)** → There **is** an effect, difference, or relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Examples of Hypothesis Pairs\n",
    "\n",
    "| Test Type                | Null Hypothesis (H₀)                        | Alternative (H₁)                            |\n",
    "|--------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| Two-sample t-test        | Mean A = Mean B                             | Mean A ≠ Mean B (or A > B, A < B)           |\n",
    "| One-sample t-test        | Mean = reference value                      | Mean ≠ reference value                      |\n",
    "| Proportions z-test       | Prop A = Prop B                             | Prop A ≠ Prop B (or A > B)                  |\n",
    "| ANOVA                    | All group means are equal                   | At least one group mean is different        |\n",
    "| Chi-square test          | Category distributions are the same         | Distributions differ across groups          |\n",
    "| McNemar’s test           | Before = After                              | Before ≠ After                              |\n",
    "\n",
    "---\n",
    "\n",
    "### 📏 One-Tailed vs Two-Tailed\n",
    "\n",
    "- **Two-tailed**: Tests for *any* difference (≠)\n",
    "- **One-tailed**: Tests for *directional* difference (> or <)\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Why Does This Matter?\n",
    "\n",
    "Framing a clear hypothesis helps:\n",
    "- Justify the test selection\n",
    "- Interpret results correctly\n",
    "- Communicate with non-technical stakeholders\n",
    "- Align on **what question** the analysis is really answering\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Step: Generate Hypothesis Statement\n",
      "\n",
      "🧭 Step: Determine Which Statistical Test to Use\n",
      "📦 Inputs:\n",
      "• Outcome Type         = `continuous`\n",
      "• Group Count          = `two-sample`\n",
      "• Group Relationship   = `independent`\n",
      "• Distribution         = `normal`\n",
      "• Equal Variance       = `equal`\n",
      "• Parametric Flag      = `True`\n",
      "\n",
      "🔍 Matching against known test cases...\n",
      "\n",
      "✅ Selected Test: `two_sample_ttest_pooled`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Selected Test        : `two_sample_ttest_pooled`\n",
      "🔍 Tail Type            : `two-tailed`\n",
      "\n",
      "📜 Hypothesis Statement:\n",
      "• H₀: The outcome (mean/proportion) is the same across groups A and B.\n",
      "• H₁: The outcome differs between groups.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_hypothesis_statement(config):\n",
    "    \"\"\"\n",
    "    Prints the null and alternative hypothesis statements based on the selected test and tail type.\n",
    "\n",
    "    This function:\n",
    "    - Uses the provided config to determine which statistical test applies\n",
    "    - Displays clear H₀ and H₁ statements tailored to the test type and direction (one-tailed vs two-tailed)\n",
    "    - Aims to bridge technical and business understanding of what is being tested\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Configuration dictionary containing at least 'tail_type' and the required fields to determine the test\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "        Prints formatted hypothesis statements directly to output\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n🧠 Step: Generate Hypothesis Statement\")\n",
    "\n",
    "    test = determine_test_to_run(config)\n",
    "    tail = config['tail_type']\n",
    "\n",
    "    print(f\"🔍 Selected Test        : `{test}`\")\n",
    "    print(f\"🔍 Tail Type            : `{tail}`\\n\")\n",
    "\n",
    "    print(\"📜 Hypothesis Statement:\")\n",
    "\n",
    "    if test == 'one_sample_ttest':\n",
    "        print(\"• H₀: The sample mean equals the reference value.\")\n",
    "        print(\"• H₁:\", \"The sample mean is different from the reference.\" if tail == 'two-tailed' else \"The sample mean is greater/less than the reference.\")\n",
    "\n",
    "    elif test == 'one_proportion_ztest':\n",
    "        print(\"• H₀: The sample proportion equals the baseline rate.\")\n",
    "        print(\"• H₁:\", \"The sample proportion is different from the baseline.\" if tail == 'two-tailed' else \"The sample proportion is greater/less than the baseline.\")\n",
    "\n",
    "    elif test in ['two_sample_ttest_pooled', 'two_sample_ttest_welch', 'mann_whitney_u', 'two_proportion_ztest']:\n",
    "        print(\"• H₀: The outcome (mean/proportion) is the same across groups A and B.\")\n",
    "        print(\"• H₁:\", \"The outcome differs between groups.\" if tail == 'two-tailed' else \"Group B is greater/less than Group A.\")\n",
    "\n",
    "    elif test in ['paired_ttest', 'wilcoxon_signed_rank']:\n",
    "        print(\"• H₀: The average difference between paired values (before vs after) is zero.\")\n",
    "        print(\"• H₁:\", \"There is a difference in paired values.\" if tail == 'two-tailed' else \"After is greater/less than before.\")\n",
    "\n",
    "    elif test == 'mcnemar':\n",
    "        print(\"• H₀: Proportion of success is the same before and after.\")\n",
    "        print(\"• H₁: Proportion of success changed after treatment.\")\n",
    "\n",
    "    elif test in ['anova', 'kruskal_wallis']:\n",
    "        print(\"• H₀: All group means (or distributions) are equal.\")\n",
    "        print(\"• H₁: At least one group differs.\")\n",
    "\n",
    "    elif test == 'chi_square':\n",
    "        print(\"• H₀: Category distributions are the same across groups.\")\n",
    "        print(\"• H₁: At least one category behaves differently.\")\n",
    "\n",
    "    elif test == 'poisson_test':\n",
    "        print(\"• H₀: The count rate (λ) is the same across groups.\")\n",
    "        print(\"• H₁: Count rate differs between groups.\")\n",
    "\n",
    "    elif test == 'bayesian_ab':\n",
    "        print(\"• Posterior: Probability that Group B is better than Group A.\")\n",
    "\n",
    "    elif test == 'permutation_test':\n",
    "        print(\"• H₀: Observed difference is due to chance.\")\n",
    "        print(\"• H₁: Observed difference is unlikely under random shuffling.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❓ Unable to generate hypothesis statement for test: `{test}`\")\n",
    "\n",
    "    display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "        \n",
    "print_hypothesis_statement(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n",
    "\n",
    "<a id=\"run-test\"></a>\n",
    "#### 🧪 Run Hypothesis Test\n",
    "\n",
    "<details><summary><strong>📖 Running the Hypothesis Test (Click to Expand)</strong></summary>\n",
    "\n",
    "### 🧠 What Happens in This Step?\n",
    "\n",
    "This function takes in your final config + dataset and **executes the appropriate test** — outputting:\n",
    "\n",
    "- The test statistic (e.g., t, z, chi², U, F)\n",
    "- The p-value\n",
    "- Whether the result is statistically significant\n",
    "\n",
    "### 🧪 Interpreting the Output\n",
    "\n",
    "| Field         | What It Means |\n",
    "|---------------|----------------|\n",
    "| `statistic`   | The test result (e.g., t-score, chi-square, etc) |\n",
    "| `p_value`     | Probability of seeing this result by chance |\n",
    "| `significant` | `True` if `p < alpha` (reject H₀), else `False` |\n",
    "| `alpha`       | The pre-defined significance threshold (typically 0.05) |\n",
    "\n",
    "### 📏 Significance Logic\n",
    "\n",
    "- If **p < alpha** → reject the null hypothesis  \n",
    "- If **p ≥ alpha** → fail to reject the null\n",
    "\n",
    "### ⚠️ Robustness\n",
    "\n",
    "The function handles different test types:\n",
    "- Parametric (e.g., t-tests, ANOVA)\n",
    "- Non-parametric (e.g., Wilcoxon, Mann-Whitney)\n",
    "- Binary proportions (e.g., z-test, McNemar)\n",
    "- Multi-group (e.g., ANOVA, chi-square)\n",
    "- Even fallback with `test_not_found`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hypothesis_test(config, df):\n",
    "    \"\"\"\n",
    "    Runs the appropriate hypothesis test based on the provided configuration and dataset.\n",
    "\n",
    "    This function:\n",
    "    - Identifies the correct test using `determine_test_to_run(config)`\n",
    "    - Executes the corresponding statistical test (e.g., t-test, z-test, Mann-Whitney, ANOVA, etc.)\n",
    "    - Prints a guided explanation of inputs, selected test, test statistic, p-value, and business interpretation\n",
    "    - Returns a result dictionary with test details and significance flag\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    config : dict\n",
    "        Dictionary specifying test configuration (e.g., outcome_type, group_relationship, parametric, etc.)\n",
    "    df : pandas.DataFrame\n",
    "        Input dataset containing outcome values (and group labels if applicable)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            'test': str,              # Name of the statistical test run\n",
    "            'statistic': float,       # Test statistic\n",
    "            'p_value': float,         # P-value of the test\n",
    "            'significant': bool,      # True if p < alpha\n",
    "            'alpha': float            # Significance threshold used\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\n🧪 Step: Run Hypothesis Test\")\n",
    "\n",
    "    test_name = determine_test_to_run(config)\n",
    "    alpha = config.get('alpha', 0.05)\n",
    "\n",
    "    print(f\"\\n🧭 Step: Determine Which Statistical Test to Use\")\n",
    "    print(\"📦 Inputs:\")\n",
    "    print(f\"• Outcome Type         = `{config['outcome_type']}`\")\n",
    "    print(f\"• Group Count          = `{config['group_count']}`\")\n",
    "    print(f\"• Group Relationship   = `{config['group_relationship']}`\")\n",
    "    print(f\"• Distribution         = `{config['distribution']}`\")\n",
    "    print(f\"• Equal Variance       = `{config['variance_equal']}`\")\n",
    "    print(f\"• Parametric Flag      = `{config['parametric']}`\")\n",
    "\n",
    "    print(f\"\\n🔍 Matching against known test cases...\")\n",
    "    print(f\"✅ Selected Test        : `{test_name}`\")\n",
    "    print(f\"🔍 Significance Threshold (α) : {alpha:.2f}\\n\")\n",
    "\n",
    "    result = {\n",
    "        'test': test_name,\n",
    "        'statistic': None,\n",
    "        'p_value': None,\n",
    "        'significant': None,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"🚀 Executing statistical test...\")\n",
    "\n",
    "        # --- Run appropriate test ---\n",
    "        if test_name == 'one_sample_ttest':\n",
    "            stat, p = ttest_1samp(df['value'], config['population_mean'])\n",
    "\n",
    "        elif test_name == 'one_sample_wilcoxon':\n",
    "            stat, p = wilcoxon(df['value'] - config['population_mean'])\n",
    "\n",
    "        elif test_name == 'one_proportion_ztest':\n",
    "            x = np.sum(df['value'])\n",
    "            n = len(df)\n",
    "            stat, p = proportions_ztest(x, n, value=config['population_mean'])\n",
    "\n",
    "        elif test_name == 'two_sample_ttest_pooled':\n",
    "            a = df[df['group'] == 'A']['value']\n",
    "            b = df[df['group'] == 'B']['value']\n",
    "            stat, p = ttest_ind(a, b, equal_var=True)\n",
    "\n",
    "        elif test_name == 'two_sample_ttest_welch':\n",
    "            a = df[df['group'] == 'A']['value']\n",
    "            b = df[df['group'] == 'B']['value']\n",
    "            stat, p = ttest_ind(a, b, equal_var=False)\n",
    "\n",
    "        elif test_name == 'mann_whitney_u':\n",
    "            a = df[df['group'] == 'A']['value']\n",
    "            b = df[df['group'] == 'B']['value']\n",
    "            stat, p = mannwhitneyu(a, b, alternative='two-sided')\n",
    "\n",
    "        elif test_name == 'paired_ttest':\n",
    "            stat, p = ttest_rel(df['group_A'], df['group_B'])\n",
    "\n",
    "        elif test_name == 'wilcoxon_signed_rank':\n",
    "            stat, p = wilcoxon(df['group_A'], df['group_B'])\n",
    "\n",
    "        elif test_name == 'two_proportion_ztest':\n",
    "            a = df[df['group'] == 'A']['value']\n",
    "            b = df[df['group'] == 'B']['value']\n",
    "            counts = [np.sum(a), np.sum(b)]\n",
    "            nobs = [len(a), len(b)]\n",
    "            stat, p = proportions_ztest(count=counts, nobs=nobs)\n",
    "\n",
    "        elif test_name == 'mcnemar':\n",
    "            before = df['group_A']\n",
    "            after = df['group_B']\n",
    "            both = np.sum((before == 1) & (after == 1))\n",
    "            before_only = np.sum((before == 1) & (after == 0))\n",
    "            after_only = np.sum((before == 0) & (after == 1))\n",
    "            neither = np.sum((before == 0) & (after == 0))\n",
    "            table = np.array([[both, before_only], [after_only, neither]])\n",
    "            stat, p = chi2_contingency(table, correction=True)[:2]\n",
    "\n",
    "        elif test_name == 'anova':\n",
    "            groups = [g['value'].values for _, g in df.groupby('group')]\n",
    "            stat, p = f_oneway(*groups)\n",
    "\n",
    "        elif test_name == 'kruskal_wallis':\n",
    "            groups = [g['value'].values for _, g in df.groupby('group')]\n",
    "            stat, p = kruskal(*groups)\n",
    "\n",
    "        elif test_name == 'chi_square':\n",
    "            contingency = pd.crosstab(df['group'], df['value'])\n",
    "            stat, p, _, _ = chi2_contingency(contingency)\n",
    "\n",
    "        else:\n",
    "            warnings.warn(f\"❌ Test not implemented: `{test_name}`\")\n",
    "            return result\n",
    "\n",
    "        result['statistic'] = stat\n",
    "        result['p_value'] = p\n",
    "        result['significant'] = p < alpha\n",
    "\n",
    "        # --- Final Output Block ---\n",
    "        print(f\"\\n📊 Test Summary: {test_name.replace('_', ' ').title()}\")\n",
    "\n",
    "        print(\"\\n🧪 Technical Result\")\n",
    "        print(f\"• Test Statistic     = {stat:.4f}\")\n",
    "        print(f\"• P-value            = {p:.4f}\")\n",
    "        print(f\"• Alpha (α)          = {alpha:.2f}\")\n",
    "\n",
    "        if p < alpha:\n",
    "            print(f\"• Conclusion         = ✅ Statistically significant → Reject H₀\")\n",
    "            print(\"\\n📈 Interpretation\")\n",
    "            print(\"• The observed difference is unlikely due to random variation.\")\n",
    "\n",
    "            # Business Interpretation\n",
    "            if config['group_count'] == 'two-sample' and config['group_relationship'] == 'independent':\n",
    "                a = df[df['group'] == 'A']['value']\n",
    "                b = df[df['group'] == 'B']['value']\n",
    "                mean_a = np.mean(a)\n",
    "                mean_b = np.mean(b)\n",
    "                lift = mean_b - mean_a\n",
    "                pct_lift = (lift / mean_a) * 100\n",
    "\n",
    "                label = \"mean\" if config['outcome_type'] == 'continuous' else 'conversion rate'\n",
    "                print(\"\\n💼 Business Insight\")\n",
    "                print(f\"• Group A {label} = {mean_a:.2f}\")\n",
    "                print(f\"• Group B {label} = {mean_b:.2f}\")\n",
    "                print(f\"• Lift = {lift:.2f} ({pct_lift:+.2f}%)\")\n",
    "\n",
    "                if lift > 0:\n",
    "                    print(\"🏆 Group B outperforms Group A — and the effect is statistically significant.\")\n",
    "                else:\n",
    "                    print(\"📉 Group B underperforms Group A — and the drop is statistically significant.\")\n",
    "        else:\n",
    "            print(f\"• Conclusion         = ❌ Not statistically significant → Fail to reject H₀\")\n",
    "            print(\"\\n📈 Interpretation\")\n",
    "            print(\"• The observed difference could be explained by randomness.\")\n",
    "            print(\"\\n💼 Business Insight\")\n",
    "            print(\"• No strong evidence of difference between the groups.\")\n",
    "\n",
    "        display(HTML(\"<hr style='border: none; height: 1px; background-color: #ddd;' />\"))\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"🚨 Error during test execution: {e}\")\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Step: Run Hypothesis Test\n",
      "\n",
      "🧭 Step: Determine Which Statistical Test to Use\n",
      "📦 Inputs:\n",
      "• Outcome Type         = `continuous`\n",
      "• Group Count          = `two-sample`\n",
      "• Group Relationship   = `independent`\n",
      "• Distribution         = `normal`\n",
      "• Equal Variance       = `equal`\n",
      "• Parametric Flag      = `True`\n",
      "\n",
      "🔍 Matching against known test cases...\n",
      "\n",
      "✅ Selected Test: `two_sample_ttest_pooled`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧭 Step: Determine Which Statistical Test to Use\n",
      "📦 Inputs:\n",
      "• Outcome Type         = `continuous`\n",
      "• Group Count          = `two-sample`\n",
      "• Group Relationship   = `independent`\n",
      "• Distribution         = `normal`\n",
      "• Equal Variance       = `equal`\n",
      "• Parametric Flag      = `True`\n",
      "\n",
      "🔍 Matching against known test cases...\n",
      "✅ Selected Test        : `two_sample_ttest_pooled`\n",
      "🔍 Significance Threshold (α) : 0.05\n",
      "\n",
      "🚀 Executing statistical test...\n",
      "\n",
      "📊 Test Summary: Two Sample Ttest Pooled\n",
      "\n",
      "🧪 Technical Result\n",
      "• Test Statistic     = -3.0942\n",
      "• P-value            = 0.0023\n",
      "• Alpha (α)          = 0.05\n",
      "• Conclusion         = ✅ Statistically significant → Reject H₀\n",
      "\n",
      "📈 Interpretation\n",
      "• The observed difference is unlikely due to random variation.\n",
      "\n",
      "💼 Business Insight\n",
      "• Group A mean = 4.92\n",
      "• Group B mean = 5.36\n",
      "• Lift = 0.45 (+9.07%)\n",
      "🏆 Group B outperforms Group A — and the effect is statistically significant.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<hr style='border: none; height: 1px; background-color: #ddd;' />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = run_hypothesis_test(config, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test-summary\"></a>\n",
    "# 📊 Test Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"full-pipeline\"></a>\n",
    "# 🚀 Full Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
