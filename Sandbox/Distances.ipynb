{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"table-of-contents\"></a>  \n",
    "# 📖 Table of Contents  \n",
    "1. [🧭 Overview](#overview)  \n",
    "2. [📏 Distance Metrics for Numeric Data](#distance-metrics-for-numeric-data)  \n",
    "   - [📌 Euclidean Distance](#euclidean-distance)  \n",
    "   - [📌 Manhattan Distance](#manhattan-distance)  \n",
    "   - [📌 Minkowski Distance](#minkowski-distance)  \n",
    "   - [📌 Mahalanobis Distance](#mahalanobis-distance)  \n",
    "3. [🧮 Distance Metrics for Vectors and Angles](#distance-metrics-for-vectors-and-angles)  \n",
    "   - [📌 Cosine Similarity / Distance](#cosine-similarity--distance)  \n",
    "4. [🔤 Distance Metrics for Categorical or Binary Data](#distance-metrics-for-categorical-or-binary-data)  \n",
    "   - [📌 Hamming Distance](#hamming-distance)  \n",
    "   - [📌 Jaccard Similarity / Distance](#jaccard-similarity--distance)  \n",
    "5. [📊 Similarity Measures for Continuous Data](#similarity-measures-for-continuous-data)  \n",
    "   - [📌 Pearson Correlation](#pearson-correlation)  \n",
    "   - [📌 Spearman Rank Correlation](#spearman-rank-correlation)  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>  \n",
    "# 🧭 Overview  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "<p>This notebook covers a wide range of <strong>distance and similarity metrics</strong> that are foundational to machine learning and statistical analysis.</p>\n",
    "\n",
    "<ul>\n",
    "  <li>📏 <strong>Distance Metrics for Numeric Data</strong>  \n",
    "    Includes Euclidean, Manhattan, Minkowski, and Mahalanobis distances—core to algorithms like KNN, clustering, and anomaly detection.\n",
    "  </li>\n",
    "  <li>🧮 <strong>Vector-Based Measures</strong>  \n",
    "    Covers Cosine similarity, useful in high-dimensional spaces like NLP and recommender systems.\n",
    "  </li>\n",
    "  <li>🔤 <strong>Distance Metrics for Categorical/Binary Data</strong>  \n",
    "    Includes Hamming and Jaccard distances, often used in matching and similarity scoring for categorical features.\n",
    "  </li>\n",
    "  <li>📊 <strong>Similarity Measures for Continuous Data</strong>  \n",
    "    Covers Pearson and Spearman correlations, essential for understanding relationships and dependencies between numeric variables.\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "<p>Each section contains:</p>\n",
    "<ul>\n",
    "  <li>Clear explanation + intuition</li>\n",
    "  <li>Mathematical formula</li>\n",
    "  <li>Clean, reproducible code implementation</li>\n",
    "</ul>\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distance-metrics-for-numeric-data\"></a>  \n",
    "# 📏 Distance Metrics for Numeric Data  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "This section includes distance metrics that operate on **numerical features**. These metrics are used when data points are represented as vectors in a continuous feature space.\n",
    "\n",
    "They form the backbone of many machine learning algorithms, particularly those that rely on geometric closeness, such as:\n",
    "\n",
    "- 📌 **K-Nearest Neighbors (KNN)**\n",
    "- 📌 **K-Means Clustering**\n",
    "- 📌 **Anomaly Detection**\n",
    "- 📌 **Distance-based recommender systems**\n",
    "\n",
    "Each metric here differs in how it defines \"closeness\"—some are sensitive to scale or outliers, while others account for data correlations.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Euclidean Distance  \n",
    "<a id=\"euclidean-distance\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "The straight-line (as-the-crow-flies) distance between two points in space. Think of it like using a ruler to measure distance on a map.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sqrt{ \\sum_{i=1}^{n} (x_i - y_i)^2 }\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Sensitive to scale differences between features  \n",
    "- Highly affected by outliers  \n",
    "- Requires normalization when features vary in range\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Used in **KNN** and **K-Means** to compute closeness  \n",
    "- In **image processing**, for comparing pixel intensities or feature embeddings  \n",
    "- Can model physical distances in **geospatial analysis** when units are aligned\n",
    "\n",
    "📝 **Notes**  \n",
    "- Assumes all features contribute equally  \n",
    "- Simple, intuitive, but not always reliable without preprocessing  \n",
    "- Can mislead in high-dimensional spaces or with unscaled features\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Manhattan Distance  \n",
    "<a id=\"manhattan-distance\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "Measures distance by summing absolute differences across dimensions. Like navigating a city grid—no diagonal shortcuts, only vertical and horizontal movement.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Less sensitive to outliers than Euclidean  \n",
    "- Still scale-dependent—normalization is recommended  \n",
    "- Can be more robust in sparse or high-dimensional settings\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Common in **recommender systems** where input vectors are high-dimensional and sparse  \n",
    "- Used in **L1-regularized models** like Lasso, which induce sparsity  \n",
    "- Helpful when minimizing absolute error is preferred (e.g., **median-based objectives**)\n",
    "\n",
    "📝 **Notes**  \n",
    "- Captures linear path cost better than Euclidean in some contexts  \n",
    "- Useful when small differences across many features matter more than large differences in a few  \n",
    "- Often performs better than Euclidean in high-dimensional, noisy data\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Minkowski Distance  \n",
    "<a id=\"minkowski-distance\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "A generalization of both Euclidean and Manhattan distances. By adjusting the parameter \\( p \\), it morphs into different distance metrics. Think of it as a flexible distance formula with a sensitivity dial.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "d(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p}\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Sensitive to the choice of \\( p \\):  \n",
    "  - \\( p = 1 \\): Manhattan Distance  \n",
    "  - \\( p = 2 \\): Euclidean Distance  \n",
    "- Higher \\( p \\) values emphasize larger deviations  \n",
    "- Still scale-dependent like its special cases\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Used in **KNN classifiers** to experiment with different notions of \"closeness\"  \n",
    "- Helpful in **model tuning**, especially when testing sensitivity to distance metrics  \n",
    "- Useful in **feature engineering pipelines** with customizable distance needs\n",
    "\n",
    "📝 **Notes**  \n",
    "- Acts as a bridge between L1 and L2 distances  \n",
    "- Not commonly used directly, but understanding it gives you control over distance behavior  \n",
    "- Can help explore robustness to outliers by adjusting \\( p \\)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Mahalanobis Distance  \n",
    "<a id=\"mahalanobis-distance\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "Measures distance between a point and a distribution, not just another point. It accounts for the variance and correlation in the data, effectively \"whitening\" the space before measuring distance.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sqrt{(x - y)^T S^{-1} (x - y)}\n",
    "$$\n",
    "\n",
    "Where \\( S \\) is the covariance matrix of the data.\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Not scale-sensitive—handles feature scaling internally via covariance  \n",
    "- Sensitive to multicollinearity or singularity in the covariance matrix  \n",
    "- Requires a well-estimated covariance matrix (large sample size helps)\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Common in **multivariate outlier detection** (e.g., fraud detection in finance)  \n",
    "- Used in **discriminant analysis** (e.g., LDA)  \n",
    "- Helpful when features are correlated, unlike Euclidean/Manhattan\n",
    "\n",
    "📝 **Notes**  \n",
    "- Allows distance to stretch/shrink based on feature correlation structure  \n",
    "- Highlights points that are far from the mean *and* unusual based on the data distribution  \n",
    "- More reliable with large, clean datasets—can break with singular or noisy covariance\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distance-metrics-for-vectors-and-angles\"></a>  \n",
    "# 🧮 Distance Metrics for Vectors and Angles  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "This section focuses on metrics that measure **angular relationships** between vectors, rather than their raw distance.\n",
    "\n",
    "These are especially useful in **high-dimensional spaces** where magnitude is less meaningful and **direction** matters more.\n",
    "\n",
    "Typical scenarios include:\n",
    "- 🧠 **NLP**: comparing TF-IDF or embedding vectors  \n",
    "- 🎧 **Recommender Systems**: user/item interaction vectors  \n",
    "- 🧬 **Similarity Scoring** in sparse or normalized datasets\n",
    "\n",
    "These metrics shine when you're more interested in **alignment** than absolute difference.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Cosine Similarity / Distance  \n",
    "<a id=\"cosine-similarity--distance\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "Measures the angle between two vectors, not their magnitude. It captures how aligned two directions are—perfect for understanding similarity in high-dimensional, sparse spaces.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "\\text{Cosine Similarity} = \\frac{\\vec{x} \\cdot \\vec{y}}{||\\vec{x}|| \\cdot ||\\vec{y}||}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Cosine Distance} = 1 - \\text{Cosine Similarity}\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Ignores magnitude, focuses only on orientation  \n",
    "- Not affected by vector scaling (e.g., multiplying a vector by 10 doesn’t change similarity)  \n",
    "- Still sensitive to dimensionality sparsity if most features are zeros\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Dominant metric in **text analysis**, especially with **TF-IDF** vectors  \n",
    "- Used in **recommender systems** to compute user-item similarity  \n",
    "- Helps detect directionally similar patterns regardless of intensity (e.g., in topic modeling)\n",
    "\n",
    "📝 **Notes**  \n",
    "- Works well when **direction matters more than magnitude**  \n",
    "- Can be misleading if vectors are zero or near-zero (need to handle edge cases)  \n",
    "- In practice, often used with high-dimensional embeddings (e.g., NLP, document matching)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"distance-metrics-for-categorical-or-binary-data\"></a>  \n",
    "# 🔤 Distance Metrics for Categorical or Binary Data  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "This section includes metrics tailored for **categorical**, **binary**, or **boolean** feature spaces—where traditional numeric distances don’t make sense.\n",
    "\n",
    "These are particularly useful when:\n",
    "- Your data is one-hot encoded  \n",
    "- You're comparing sequences, strings, or sets  \n",
    "- Features are **non-numeric** but still informative\n",
    "\n",
    "Common applications:\n",
    "- 🧬 **Genomic and text sequence comparison**  \n",
    "- 📦 **Product recommendation based on binary attributes**  \n",
    "- 🏷️ **Clustering with categorical features**  \n",
    "\n",
    "These metrics help quantify **presence/absence** and **set overlap**, making them ideal for discrete comparisons.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Hamming Distance  \n",
    "<a id=\"hamming-distance\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "Counts how many positions two strings (or binary vectors) differ in. Imagine comparing two passwords or binary sequences and marking the mismatches.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "d(x, y) = \\sum_{i=1}^{n} \\mathbf{1}(x_i \\ne y_i) \\\\\n",
    "\\text{where } \\mathbf{1}(x_i \\ne y_i) = \n",
    "\\begin{cases}\n",
    "1, & \\text{if } x_i \\ne y_i \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Only works on equal-length vectors  \n",
    "- Binary/categorical only—makes no sense for continuous values  \n",
    "- Each mismatch is treated equally, no weighting\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Used in **error detection/correction** (e.g., digital communication, QR codes)  \n",
    "- Common in **genomic sequence analysis**  \n",
    "- Helpful for comparing **one-hot encoded categorical features** in clustering or similarity scoring\n",
    "\n",
    "📝 **Notes**  \n",
    "- Simple and interpretable for binary comparisons  \n",
    "- Doesn’t account for *how different* the values are—just whether they differ  \n",
    "- Can be extended to non-binary categorical data using matching scores\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Jaccard Similarity / Distance  \n",
    "<a id=\"jaccard-similarity--distance\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "Measures the overlap between two sets relative to their union. It tells you *how similar two binary vectors or sets are*, ignoring what they don't share.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "\\text{Jaccard Similarity} = \\frac{|A \\cap B|}{|A \\cup B|} \\\\\n",
    "\\text{Jaccard Distance} = 1 - \\text{Jaccard Similarity}\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Only works on binary/categorical data or sets  \n",
    "- Ignores true negatives (things both sets don't have)  \n",
    "- Sensitive to sparsity—more zeros → lower similarity\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Common in **recommender systems** to compare item sets (e.g., users with similar purchase histories)  \n",
    "- Used in **clustering binary data** (e.g., one-hot encoded attributes)  \n",
    "- Applied in **text mining** to compare sets of words (bag-of-words or shingled phrases)\n",
    "\n",
    "📝 **Notes**  \n",
    "- Especially useful when **presence** is more important than absence  \n",
    "- Performs well when comparing sparse or asymmetric binary vectors  \n",
    "- Jaccard Distance is a proper metric (satisfies triangle inequality)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"similarity-measures-for-continuous-data\"></a>  \n",
    "# 📊 Similarity Measures for Continuous Data  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "This section covers **correlation-based similarity measures** for continuous variables. Instead of measuring distance, these metrics quantify the **strength and direction of relationships** between variables.\n",
    "\n",
    "Use cases typically involve:\n",
    "- 📈 **Exploratory Data Analysis (EDA)**  \n",
    "- 🧪 **Feature selection** in modeling pipelines  \n",
    "- 💰 **Financial modeling** (e.g., correlation between asset returns)\n",
    "\n",
    "These measures are:\n",
    "- Scale-invariant  \n",
    "- Useful for spotting patterns in **paired continuous variables**  \n",
    "- Sensitive to relationship type—linear vs. monotonic\n",
    "\n",
    "These metrics are key to understanding **how variables move together**, whether for modeling or diagnostics.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Pearson Correlation  \n",
    "<a id=\"pearson-correlation\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "Measures the strength and direction of a **linear relationship** between two continuous variables. A value of +1 means perfect positive linear correlation, -1 means perfect negative, and 0 means no linear relationship.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}\n",
    "         {\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}}\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Extremely sensitive to **outliers**  \n",
    "- Assumes **linearity**  \n",
    "- Affected by non-normal distributions or non-constant variance\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Used in **feature selection** (e.g., removing highly correlated variables)  \n",
    "- Helps in **exploratory data analysis** to understand relationships  \n",
    "- Common in **finance** (e.g., correlation between stock returns)\n",
    "\n",
    "📝 **Notes**  \n",
    "- Does **not imply causation**—only association  \n",
    "- Works best when both variables are continuous, normally distributed, and linearly related  \n",
    "- For non-linear relationships, consider Spearman instead\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 📌 Spearman Rank Correlation  \n",
    "<a id=\"spearman-rank-correlation\"></a>  \n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>  \n",
    "\n",
    "🧠 **Intuition**  \n",
    "Measures the **monotonic relationship** between two variables using their ranks instead of raw values. It tells you whether the relationship is consistently increasing or decreasing, even if not linear.\n",
    "\n",
    "🧮 **Formula**\n",
    "\n",
    "If there are no tied ranks:\n",
    "\n",
    "$$\n",
    "\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)} \\\\\n",
    "\\text{where } d_i = \\text{rank}(x_i) - \\text{rank}(y_i)\n",
    "$$\n",
    "\n",
    "⚠️ **Sensitivity**  \n",
    "- Robust to **outliers**  \n",
    "- Captures **monotonic** (not just linear) trends  \n",
    "- Still assumes **ordinal** or continuous variables\n",
    "\n",
    "🧰 **Use Cases + Real-World Examples**  \n",
    "- Great for **ordinal data** (e.g., survey rankings, Likert scales)  \n",
    "- Used when variables don’t meet normality assumptions  \n",
    "- Common in **bioinformatics** or **psychometrics** for measuring association strength\n",
    "\n",
    "📝 **Notes**  \n",
    "- Doesn’t assume linearity or equal spacing between values  \n",
    "- Less powerful than Pearson when linearity holds  \n",
    "- Ideal fallback when data violates Pearson’s assumptions\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Metrics:\n",
    "\tEuclidean Distance: The straight-line distance between two points in space.\n",
    "\tManhattan Distance: Also called L1 distance, it measures the sum of absolute differences.\n",
    "\tMinkowski Distance: A generalization of Euclidean and Manhattan distance.\n",
    "\tCosine Similarity: Measures the cosine of the angle between two vectors, often used in text analysis.\n",
    "\tMahalanobis Distance: Takes into account the correlations of the data set and is useful for multivariate analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Measures:\n",
    "\tJaccard Similarity: Measures similarity between finite sample sets, used for binary attributes.\n",
    "\tPearson Correlation: Measures linear correlation between two variables.\n",
    "\tSpearman Rank Correlation: Measures the relationship between two variables using rank-order.\n",
    "\tHamming Distance: Used to compare strings of equal length, measuring the number of positions at which the corresponding elements are different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
