{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization & Convergence:\n",
    "\tGradient Descent: Fundamental for many machine learning algorithms, understanding how this optimization method converges.\n",
    "\tConvexity: Fundamental in optimization. Knowing when a problem is convex makes optimization easier to understand.\n",
    "\tConvexity in Optimization: Knowing if an objective function is convex can determine if optimization problems have global solutions.\n",
    "\tLagrange Multipliers: These are fundamental for constrained optimization problems, which frequently arise in machine learning tasks.\n",
    "\tDuality Theory: In optimization, duality can provide insights into the problem's structure and make it easier to solve complex problems.\n",
    "\tGradient Descent Variants: Stochastic, mini-batch, and batch gradient descent â€” understanding their trade-offs is essential in model training.\n",
    "\tAdam Optimizer: Combines the advantages of two other extensions of gradient descent, AdaGrad and RMSProp. Crucial for deep learning.\n",
    "\tSimulated Annealing: A probabilistic technique for approximating the global optimum of a given function.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
